{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mask-rcnn training notebook.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kt_U4hv5oYV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "outputId": "81dffdf1-ddf8-4a67-ff35-e3d498dce099"
      },
      "source": [
        "%%bash\n",
        "\n",
        "git clone https://github.com/matterport/Mask_RCNN.git\n",
        "cd Mask_RCNN\n",
        "pip install -r requirements.txt -q\n",
        "python setup.py install\n",
        "cd .."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating mask_rcnn.egg-info\n",
            "writing mask_rcnn.egg-info/PKG-INFO\n",
            "writing dependency_links to mask_rcnn.egg-info/dependency_links.txt\n",
            "writing top-level names to mask_rcnn.egg-info/top_level.txt\n",
            "writing manifest file 'mask_rcnn.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "writing manifest file 'mask_rcnn.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/mrcnn\n",
            "copying mrcnn/utils.py -> build/lib/mrcnn\n",
            "copying mrcnn/__init__.py -> build/lib/mrcnn\n",
            "copying mrcnn/model.py -> build/lib/mrcnn\n",
            "copying mrcnn/config.py -> build/lib/mrcnn\n",
            "copying mrcnn/parallel_model.py -> build/lib/mrcnn\n",
            "copying mrcnn/visualize.py -> build/lib/mrcnn\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/utils.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/__init__.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/model.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/config.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/parallel_model.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/visualize.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/utils.py to utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/model.py to model.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/config.py to config.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/parallel_model.py to parallel_model.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/visualize.py to visualize.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "creating dist\n",
            "creating 'dist/mask_rcnn-2.1-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing mask_rcnn-2.1-py3.6.egg\n",
            "Copying mask_rcnn-2.1-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding mask-rcnn 2.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg\n",
            "Processing dependencies for mask-rcnn==2.1\n",
            "Finished processing dependencies for mask-rcnn==2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Mask_RCNN'...\n",
            "WARNING:root:Fail load requirements file, so using default ones.\n",
            "zip_safe flag not set; analyzing archive contents...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkxelHcm6USN",
        "colab_type": "text"
      },
      "source": [
        "### Get [pre-trained](https://www.dropbox.com/s/1kql7tsug876xfn/kaggle_bowl.h5?dl=0) weight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox-rj94H5vpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFc142M-50E5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "\n",
        "mkdir Mask_RCNN/weights\n",
        "cp drive/My\\ Drive/kaggle_bowl.h5 Mask_RCNN/weights/\n",
        "cp drive/My\\ Drive/nuclei_datasets.tar.gz Mask_RCNN/\n",
        "python -m tarfile -e Mask_RCNN/nuclei_datasets.tar.gz Mask_RCNN/\n",
        "\n",
        "mkdir Mask_RCNN/nuclei_datasets/stage1_test\n",
        "mkdir Mask_RCNN/nuclei_datasets/stage1_train\n",
        "python -m zipfile -e Mask_RCNN/nuclei_datasets/stage1_test.zip Mask_RCNN/nuclei_datasets/stage1_test\n",
        "python -m zipfile -e Mask_RCNN/nuclei_datasets/stage1_train.zip Mask_RCNN/nuclei_datasets/stage1_train\n",
        "\n",
        "rm Mask_RCNN/nuclei_datasets/*.zip\n",
        "rm Mask_RCNN/nuclei_datasets.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED9WBbPz6A4J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "db8bff5a-ec5a-4ada-d919-f7df93f5934c"
      },
      "source": [
        "!python Mask_RCNN/samples/nucleus/nucleus.py train \\\n",
        "    --dataset=Mask_RCNN/nuclei_datasets --subset=stage1_train \\\n",
        "    --weights=Mask_RCNN/weights/kaggle_bowl.h5"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "Weights:  Mask_RCNN/weights/kaggle_bowl.h5\n",
            "Dataset:  Mask_RCNN/nuclei_datasets\n",
            "Subset:  stage1_train\n",
            "Logs:  /logs\n",
            "\n",
            "Configurations:\n",
            "BACKBONE                       resnet50\n",
            "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
            "BATCH_SIZE                     6\n",
            "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
            "COMPUTE_BACKBONE_SHAPE         None\n",
            "DETECTION_MAX_INSTANCES        400\n",
            "DETECTION_MIN_CONFIDENCE       0\n",
            "DETECTION_NMS_THRESHOLD        0.3\n",
            "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
            "GPU_COUNT                      1\n",
            "GRADIENT_CLIP_NORM             5.0\n",
            "IMAGES_PER_GPU                 6\n",
            "IMAGE_CHANNEL_COUNT            3\n",
            "IMAGE_MAX_DIM                  512\n",
            "IMAGE_META_SIZE                14\n",
            "IMAGE_MIN_DIM                  512\n",
            "IMAGE_MIN_SCALE                2.0\n",
            "IMAGE_RESIZE_MODE              crop\n",
            "IMAGE_SHAPE                    [512 512   3]\n",
            "LEARNING_MOMENTUM              0.9\n",
            "LEARNING_RATE                  0.001\n",
            "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
            "MASK_POOL_SIZE                 14\n",
            "MASK_SHAPE                     [28, 28]\n",
            "MAX_GT_INSTANCES               200\n",
            "MEAN_PIXEL                     [43.53 39.56 48.22]\n",
            "MINI_MASK_SHAPE                (56, 56)\n",
            "NAME                           nucleus\n",
            "NUM_CLASSES                    2\n",
            "POOL_SIZE                      7\n",
            "POST_NMS_ROIS_INFERENCE        2000\n",
            "POST_NMS_ROIS_TRAINING         1000\n",
            "PRE_NMS_LIMIT                  6000\n",
            "ROI_POSITIVE_RATIO             0.33\n",
            "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
            "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
            "RPN_ANCHOR_STRIDE              1\n",
            "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
            "RPN_NMS_THRESHOLD              0.9\n",
            "RPN_TRAIN_ANCHORS_PER_IMAGE    64\n",
            "STEPS_PER_EPOCH                105\n",
            "TOP_DOWN_PYRAMID_SIZE          256\n",
            "TRAIN_BN                       False\n",
            "TRAIN_ROIS_PER_IMAGE           128\n",
            "USE_MINI_MASK                  True\n",
            "USE_RPN_ROIS                   True\n",
            "VALIDATION_STEPS               4\n",
            "WEIGHT_DECAY                   0.0001\n",
            "\n",
            "\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0626 17:49:56.569819 139724636510080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0626 17:49:56.628535 139724636510080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0626 17:49:56.659019 139724636510080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0626 17:49:56.699804 139724636510080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1919: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0626 17:49:56.701949 139724636510080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0626 17:49:58.324085 139724636510080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "W0626 17:49:59.692348 139724636510080 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0626 17:49:59.863655 139724636510080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py:553: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.\n",
            "\n",
            "W0626 17:49:59.943818 139724636510080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0626 17:49:59.982014 139724636510080 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py:600: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "box_ind is deprecated, use box_indices instead\n",
            "Loading weights  Mask_RCNN/weights/kaggle_bowl.h5\n",
            "2019-06-26 17:50:04.070196: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-26 17:50:04.072264: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14378840 executing computations on platform Host. Devices:\n",
            "2019-06-26 17:50:04.072302: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-26 17:50:04.080695: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-26 17:50:04.254362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-26 17:50:04.254940: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14378bc0 executing computations on platform CUDA. Devices:\n",
            "2019-06-26 17:50:04.254977: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-06-26 17:50:04.255219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-26 17:50:04.255621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-26 17:50:04.276553: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-26 17:50:04.473190: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-26 17:50:04.558453: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-26 17:50:04.586231: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-26 17:50:04.800465: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-26 17:50:04.930784: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-26 17:50:05.337632: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-26 17:50:05.337922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-26 17:50:05.338541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-26 17:50:05.338922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-26 17:50:05.343344: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-26 17:50:05.345852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-26 17:50:05.345893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-26 17:50:05.345924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-26 17:50:05.348252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-26 17:50:05.348776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-26 17:50:05.349175: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-06-26 17:50:05.349239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10802 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "Train network heads\n",
            "\n",
            "Starting at epoch 0. LR=0.001\n",
            "\n",
            "Checkpoint Path: /logs/nucleus20190626T1750/mask_rcnn_nucleus_{epoch:04d}.h5\n",
            "Selecting layers to train\n",
            "fpn_c5p5               (Conv2D)\n",
            "fpn_c4p4               (Conv2D)\n",
            "fpn_c3p3               (Conv2D)\n",
            "fpn_c2p2               (Conv2D)\n",
            "fpn_p5                 (Conv2D)\n",
            "fpn_p2                 (Conv2D)\n",
            "fpn_p3                 (Conv2D)\n",
            "fpn_p4                 (Conv2D)\n",
            "In model:  rpn_model\n",
            "    rpn_conv_shared        (Conv2D)\n",
            "    rpn_class_raw          (Conv2D)\n",
            "    rpn_bbox_pred          (Conv2D)\n",
            "mrcnn_mask_conv1       (TimeDistributed)\n",
            "mrcnn_mask_bn1         (TimeDistributed)\n",
            "mrcnn_mask_conv2       (TimeDistributed)\n",
            "mrcnn_mask_bn2         (TimeDistributed)\n",
            "mrcnn_class_conv1      (TimeDistributed)\n",
            "mrcnn_class_bn1        (TimeDistributed)\n",
            "mrcnn_mask_conv3       (TimeDistributed)\n",
            "mrcnn_mask_bn3         (TimeDistributed)\n",
            "mrcnn_class_conv2      (TimeDistributed)\n",
            "mrcnn_class_bn2        (TimeDistributed)\n",
            "mrcnn_mask_conv4       (TimeDistributed)\n",
            "mrcnn_mask_bn4         (TimeDistributed)\n",
            "mrcnn_bbox_fc          (TimeDistributed)\n",
            "mrcnn_mask_deconv      (TimeDistributed)\n",
            "mrcnn_class_logits     (TimeDistributed)\n",
            "mrcnn_mask             (TimeDistributed)\n",
            "W0626 17:50:11.864149 139724636510080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
            "  UserWarning('Using a generator with `use_multiprocessing=True`'\n",
            "W0626 17:50:19.603641 139724636510080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "W0626 17:50:19.604025 139724636510080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "Epoch 1/20\n",
            "2019-06-26 17:51:02.138436: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-06-26 17:51:04.507969: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-26 17:51:05.494343: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-26 17:51:16.919153: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.60GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            "2019-06-26 17:51:19.706987: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.30GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            " 10/105 [=>............................] - ETA: 25:12 - loss: 0.9861 - rpn_class_loss: 0.1010 - rpn_bbox_loss: 0.2859 - mrcnn_class_loss: 0.2321 - mrcnn_bbox_loss: 0.1423 - mrcnn_mask_loss: 0.2248tcmalloc: large alloc 1150525440 bytes == 0x4e964000 @  0x7f142d102001 0x7f142956fde5 0x7f14295d46f1 0x7f14295d67cf 0x7f142966f158 0x4f8925 0x4fa6c0 0x4f6128 0x4f7d60 0x4f876d 0x4fa6c0 0x4f6128 0x4f7d60 0x4f876d 0x4fa6c0 0x4f6128 0x4f7d60 0x4f876d 0x4f98c7 0x4f6128 0x4f7d60 0x4f876d 0x4fa6c0 0x59c333 0x5017cf 0x4f858d 0x4f98c7 0x4f6128 0x56fe24 0x57c2fe 0x4facb1\n",
            "tcmalloc: large alloc 1150525440 bytes == 0x4e964000 @  0x7f142d102001 0x7f142956fde5 0x7f14295d46f1 0x7f14295d67cf 0x7f142966f158 0x4f8925 0x4fa6c0 0x4f6128 0x4f7d60 0x4f876d 0x4fa6c0 0x4f6128 0x4f7d60 0x4f876d 0x4fa6c0 0x4f6128 0x4f7d60 0x4f876d 0x4f98c7 0x4f6128 0x4f7d60 0x4f876d 0x4fa6c0 0x59c333 0x5017cf 0x4f858d 0x4f98c7 0x4f6128 0x56fe24 0x57c2fe 0x4facb1\n",
            "105/105 [==============================] - 1426s 14s/step - loss: 0.8951 - rpn_class_loss: 0.0531 - rpn_bbox_loss: 0.2719 - mrcnn_class_loss: 0.2102 - mrcnn_bbox_loss: 0.1371 - mrcnn_mask_loss: 0.2227 - val_loss: 1.0824 - val_rpn_class_loss: 0.0506 - val_rpn_bbox_loss: 0.3561 - val_mrcnn_class_loss: 0.2329 - val_mrcnn_bbox_loss: 0.1850 - val_mrcnn_mask_loss: 0.2578\n",
            "Epoch 2/20\n",
            "105/105 [==============================] - 1014s 10s/step - loss: 0.8283 - rpn_class_loss: 0.0429 - rpn_bbox_loss: 0.2622 - mrcnn_class_loss: 0.1918 - mrcnn_bbox_loss: 0.1239 - mrcnn_mask_loss: 0.2076 - val_loss: 1.0128 - val_rpn_class_loss: 0.0446 - val_rpn_bbox_loss: 0.2747 - val_mrcnn_class_loss: 0.2792 - val_mrcnn_bbox_loss: 0.1701 - val_mrcnn_mask_loss: 0.2442\n",
            "Epoch 3/20\n",
            "105/105 [==============================] - 846s 8s/step - loss: 0.8319 - rpn_class_loss: 0.0369 - rpn_bbox_loss: 0.2552 - mrcnn_class_loss: 0.1973 - mrcnn_bbox_loss: 0.1294 - mrcnn_mask_loss: 0.2131 - val_loss: 1.0479 - val_rpn_class_loss: 0.0478 - val_rpn_bbox_loss: 0.3298 - val_mrcnn_class_loss: 0.2347 - val_mrcnn_bbox_loss: 0.1751 - val_mrcnn_mask_loss: 0.2605\n",
            "Epoch 4/20\n",
            "105/105 [==============================] - 721s 7s/step - loss: 0.8028 - rpn_class_loss: 0.0361 - rpn_bbox_loss: 0.2386 - mrcnn_class_loss: 0.1890 - mrcnn_bbox_loss: 0.1277 - mrcnn_mask_loss: 0.2114 - val_loss: 1.0358 - val_rpn_class_loss: 0.0486 - val_rpn_bbox_loss: 0.3264 - val_mrcnn_class_loss: 0.2478 - val_mrcnn_bbox_loss: 0.1694 - val_mrcnn_mask_loss: 0.2436\n",
            "Epoch 5/20\n",
            "105/105 [==============================] - 792s 8s/step - loss: 0.8110 - rpn_class_loss: 0.0357 - rpn_bbox_loss: 0.2481 - mrcnn_class_loss: 0.1913 - mrcnn_bbox_loss: 0.1254 - mrcnn_mask_loss: 0.2105 - val_loss: 1.1330 - val_rpn_class_loss: 0.0521 - val_rpn_bbox_loss: 0.3226 - val_mrcnn_class_loss: 0.2985 - val_mrcnn_bbox_loss: 0.1923 - val_mrcnn_mask_loss: 0.2675\n",
            "Epoch 6/20\n",
            "105/105 [==============================] - 734s 7s/step - loss: 0.7853 - rpn_class_loss: 0.0336 - rpn_bbox_loss: 0.2283 - mrcnn_class_loss: 0.1878 - mrcnn_bbox_loss: 0.1250 - mrcnn_mask_loss: 0.2105 - val_loss: 0.8650 - val_rpn_class_loss: 0.0325 - val_rpn_bbox_loss: 0.2932 - val_mrcnn_class_loss: 0.1925 - val_mrcnn_bbox_loss: 0.1367 - val_mrcnn_mask_loss: 0.2100\n",
            "Epoch 7/20\n",
            "105/105 [==============================] - 801s 8s/step - loss: 0.7720 - rpn_class_loss: 0.0324 - rpn_bbox_loss: 0.2242 - mrcnn_class_loss: 0.1828 - mrcnn_bbox_loss: 0.1234 - mrcnn_mask_loss: 0.2092 - val_loss: 0.9999 - val_rpn_class_loss: 0.0448 - val_rpn_bbox_loss: 0.2946 - val_mrcnn_class_loss: 0.2472 - val_mrcnn_bbox_loss: 0.1677 - val_mrcnn_mask_loss: 0.2457\n",
            "Epoch 8/20\n",
            "105/105 [==============================] - 770s 7s/step - loss: 0.8024 - rpn_class_loss: 0.0325 - rpn_bbox_loss: 0.2495 - mrcnn_class_loss: 0.1868 - mrcnn_bbox_loss: 0.1254 - mrcnn_mask_loss: 0.2081 - val_loss: 0.9182 - val_rpn_class_loss: 0.0328 - val_rpn_bbox_loss: 0.2536 - val_mrcnn_class_loss: 0.2265 - val_mrcnn_bbox_loss: 0.1675 - val_mrcnn_mask_loss: 0.2378\n",
            "Epoch 9/20\n",
            "105/105 [==============================] - 725s 7s/step - loss: 0.7958 - rpn_class_loss: 0.0330 - rpn_bbox_loss: 0.2238 - mrcnn_class_loss: 0.1922 - mrcnn_bbox_loss: 0.1313 - mrcnn_mask_loss: 0.2156 - val_loss: 0.8474 - val_rpn_class_loss: 0.0314 - val_rpn_bbox_loss: 0.2480 - val_mrcnn_class_loss: 0.1929 - val_mrcnn_bbox_loss: 0.1465 - val_mrcnn_mask_loss: 0.2287\n",
            "Epoch 10/20\n",
            "105/105 [==============================] - 763s 7s/step - loss: 0.7661 - rpn_class_loss: 0.0328 - rpn_bbox_loss: 0.2216 - mrcnn_class_loss: 0.1818 - mrcnn_bbox_loss: 0.1215 - mrcnn_mask_loss: 0.2084 - val_loss: 1.1114 - val_rpn_class_loss: 0.0448 - val_rpn_bbox_loss: 0.3849 - val_mrcnn_class_loss: 0.2587 - val_mrcnn_bbox_loss: 0.1683 - val_mrcnn_mask_loss: 0.2548\n",
            "Epoch 11/20\n",
            "105/105 [==============================] - 738s 7s/step - loss: 0.7660 - rpn_class_loss: 0.0285 - rpn_bbox_loss: 0.2274 - mrcnn_class_loss: 0.1795 - mrcnn_bbox_loss: 0.1218 - mrcnn_mask_loss: 0.2088 - val_loss: 0.9646 - val_rpn_class_loss: 0.0461 - val_rpn_bbox_loss: 0.2673 - val_mrcnn_class_loss: 0.2390 - val_mrcnn_bbox_loss: 0.1647 - val_mrcnn_mask_loss: 0.2476\n",
            "Epoch 12/20\n",
            "105/105 [==============================] - 805s 8s/step - loss: 0.7823 - rpn_class_loss: 0.0342 - rpn_bbox_loss: 0.2205 - mrcnn_class_loss: 0.1868 - mrcnn_bbox_loss: 0.1282 - mrcnn_mask_loss: 0.2126 - val_loss: 0.9016 - val_rpn_class_loss: 0.0245 - val_rpn_bbox_loss: 0.2879 - val_mrcnn_class_loss: 0.2152 - val_mrcnn_bbox_loss: 0.1485 - val_mrcnn_mask_loss: 0.2255\n",
            "Epoch 13/20\n",
            "105/105 [==============================] - 776s 7s/step - loss: 0.7628 - rpn_class_loss: 0.0341 - rpn_bbox_loss: 0.2296 - mrcnn_class_loss: 0.1753 - mrcnn_bbox_loss: 0.1183 - mrcnn_mask_loss: 0.2056 - val_loss: 0.9751 - val_rpn_class_loss: 0.0392 - val_rpn_bbox_loss: 0.2773 - val_mrcnn_class_loss: 0.2543 - val_mrcnn_bbox_loss: 0.1680 - val_mrcnn_mask_loss: 0.2363\n",
            "Epoch 14/20\n",
            "105/105 [==============================] - 760s 7s/step - loss: 0.7499 - rpn_class_loss: 0.0319 - rpn_bbox_loss: 0.2172 - mrcnn_class_loss: 0.1752 - mrcnn_bbox_loss: 0.1200 - mrcnn_mask_loss: 0.2057 - val_loss: 0.8474 - val_rpn_class_loss: 0.0270 - val_rpn_bbox_loss: 0.2319 - val_mrcnn_class_loss: 0.2220 - val_mrcnn_bbox_loss: 0.1467 - val_mrcnn_mask_loss: 0.2198\n",
            "Epoch 15/20\n",
            "105/105 [==============================] - 820s 8s/step - loss: 0.7689 - rpn_class_loss: 0.0310 - rpn_bbox_loss: 0.2160 - mrcnn_class_loss: 0.1851 - mrcnn_bbox_loss: 0.1246 - mrcnn_mask_loss: 0.2122 - val_loss: 0.8846 - val_rpn_class_loss: 0.0267 - val_rpn_bbox_loss: 0.2376 - val_mrcnn_class_loss: 0.2150 - val_mrcnn_bbox_loss: 0.1669 - val_mrcnn_mask_loss: 0.2384\n",
            "Epoch 16/20\n",
            "105/105 [==============================] - 726s 7s/step - loss: 0.7636 - rpn_class_loss: 0.0324 - rpn_bbox_loss: 0.2207 - mrcnn_class_loss: 0.1782 - mrcnn_bbox_loss: 0.1227 - mrcnn_mask_loss: 0.2096 - val_loss: 0.8628 - val_rpn_class_loss: 0.0225 - val_rpn_bbox_loss: 0.2600 - val_mrcnn_class_loss: 0.2063 - val_mrcnn_bbox_loss: 0.1484 - val_mrcnn_mask_loss: 0.2255\n",
            "Epoch 17/20\n",
            "105/105 [==============================] - 742s 7s/step - loss: 0.7575 - rpn_class_loss: 0.0302 - rpn_bbox_loss: 0.2121 - mrcnn_class_loss: 0.1826 - mrcnn_bbox_loss: 0.1227 - mrcnn_mask_loss: 0.2099 - val_loss: 0.9215 - val_rpn_class_loss: 0.0338 - val_rpn_bbox_loss: 0.2374 - val_mrcnn_class_loss: 0.2450 - val_mrcnn_bbox_loss: 0.1584 - val_mrcnn_mask_loss: 0.2469\n",
            "Epoch 18/20\n",
            " 93/105 [=========================>....] - ETA: 1:32 - loss: 0.7555 - rpn_class_loss: 0.0297 - rpn_bbox_loss: 0.2164 - mrcnn_class_loss: 0.1793 - mrcnn_bbox_loss: 0.1212 - mrcnn_mask_loss: 0.20892019-06-26 21:58:02.315057: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.30GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            "105/105 [==============================] - 807s 8s/step - loss: 0.7467 - rpn_class_loss: 0.0291 - rpn_bbox_loss: 0.2136 - mrcnn_class_loss: 0.1764 - mrcnn_bbox_loss: 0.1195 - mrcnn_mask_loss: 0.2080 - val_loss: 0.9973 - val_rpn_class_loss: 0.0391 - val_rpn_bbox_loss: 0.2944 - val_mrcnn_class_loss: 0.2433 - val_mrcnn_bbox_loss: 0.1793 - val_mrcnn_mask_loss: 0.2411\n",
            "Epoch 19/20\n",
            "105/105 [==============================] - 764s 7s/step - loss: 0.7719 - rpn_class_loss: 0.0335 - rpn_bbox_loss: 0.2231 - mrcnn_class_loss: 0.1835 - mrcnn_bbox_loss: 0.1230 - mrcnn_mask_loss: 0.2088 - val_loss: 0.9753 - val_rpn_class_loss: 0.0428 - val_rpn_bbox_loss: 0.2447 - val_mrcnn_class_loss: 0.2585 - val_mrcnn_bbox_loss: 0.1800 - val_mrcnn_mask_loss: 0.2493\n",
            "Epoch 20/20\n",
            "105/105 [==============================] - 847s 8s/step - loss: 0.7685 - rpn_class_loss: 0.0326 - rpn_bbox_loss: 0.2131 - mrcnn_class_loss: 0.1850 - mrcnn_bbox_loss: 0.1251 - mrcnn_mask_loss: 0.2127 - val_loss: 0.8279 - val_rpn_class_loss: 0.0282 - val_rpn_bbox_loss: 0.2270 - val_mrcnn_class_loss: 0.2093 - val_mrcnn_bbox_loss: 0.1394 - val_mrcnn_mask_loss: 0.2240\n",
            "Train all layers\n",
            "\n",
            "Starting at epoch 20. LR=0.001\n",
            "\n",
            "Checkpoint Path: /logs/nucleus20190626T1750/mask_rcnn_nucleus_{epoch:04d}.h5\n",
            "Selecting layers to train\n",
            "conv1                  (Conv2D)\n",
            "bn_conv1               (BatchNorm)\n",
            "res2a_branch2a         (Conv2D)\n",
            "bn2a_branch2a          (BatchNorm)\n",
            "res2a_branch2b         (Conv2D)\n",
            "bn2a_branch2b          (BatchNorm)\n",
            "res2a_branch2c         (Conv2D)\n",
            "res2a_branch1          (Conv2D)\n",
            "bn2a_branch2c          (BatchNorm)\n",
            "bn2a_branch1           (BatchNorm)\n",
            "res2b_branch2a         (Conv2D)\n",
            "bn2b_branch2a          (BatchNorm)\n",
            "res2b_branch2b         (Conv2D)\n",
            "bn2b_branch2b          (BatchNorm)\n",
            "res2b_branch2c         (Conv2D)\n",
            "bn2b_branch2c          (BatchNorm)\n",
            "res2c_branch2a         (Conv2D)\n",
            "bn2c_branch2a          (BatchNorm)\n",
            "res2c_branch2b         (Conv2D)\n",
            "bn2c_branch2b          (BatchNorm)\n",
            "res2c_branch2c         (Conv2D)\n",
            "bn2c_branch2c          (BatchNorm)\n",
            "res3a_branch2a         (Conv2D)\n",
            "bn3a_branch2a          (BatchNorm)\n",
            "res3a_branch2b         (Conv2D)\n",
            "bn3a_branch2b          (BatchNorm)\n",
            "res3a_branch2c         (Conv2D)\n",
            "res3a_branch1          (Conv2D)\n",
            "bn3a_branch2c          (BatchNorm)\n",
            "bn3a_branch1           (BatchNorm)\n",
            "res3b_branch2a         (Conv2D)\n",
            "bn3b_branch2a          (BatchNorm)\n",
            "res3b_branch2b         (Conv2D)\n",
            "bn3b_branch2b          (BatchNorm)\n",
            "res3b_branch2c         (Conv2D)\n",
            "bn3b_branch2c          (BatchNorm)\n",
            "res3c_branch2a         (Conv2D)\n",
            "bn3c_branch2a          (BatchNorm)\n",
            "res3c_branch2b         (Conv2D)\n",
            "bn3c_branch2b          (BatchNorm)\n",
            "res3c_branch2c         (Conv2D)\n",
            "bn3c_branch2c          (BatchNorm)\n",
            "res3d_branch2a         (Conv2D)\n",
            "bn3d_branch2a          (BatchNorm)\n",
            "res3d_branch2b         (Conv2D)\n",
            "bn3d_branch2b          (BatchNorm)\n",
            "res3d_branch2c         (Conv2D)\n",
            "bn3d_branch2c          (BatchNorm)\n",
            "res4a_branch2a         (Conv2D)\n",
            "bn4a_branch2a          (BatchNorm)\n",
            "res4a_branch2b         (Conv2D)\n",
            "bn4a_branch2b          (BatchNorm)\n",
            "res4a_branch2c         (Conv2D)\n",
            "res4a_branch1          (Conv2D)\n",
            "bn4a_branch2c          (BatchNorm)\n",
            "bn4a_branch1           (BatchNorm)\n",
            "res4b_branch2a         (Conv2D)\n",
            "bn4b_branch2a          (BatchNorm)\n",
            "res4b_branch2b         (Conv2D)\n",
            "bn4b_branch2b          (BatchNorm)\n",
            "res4b_branch2c         (Conv2D)\n",
            "bn4b_branch2c          (BatchNorm)\n",
            "res4c_branch2a         (Conv2D)\n",
            "bn4c_branch2a          (BatchNorm)\n",
            "res4c_branch2b         (Conv2D)\n",
            "bn4c_branch2b          (BatchNorm)\n",
            "res4c_branch2c         (Conv2D)\n",
            "bn4c_branch2c          (BatchNorm)\n",
            "res4d_branch2a         (Conv2D)\n",
            "bn4d_branch2a          (BatchNorm)\n",
            "res4d_branch2b         (Conv2D)\n",
            "bn4d_branch2b          (BatchNorm)\n",
            "res4d_branch2c         (Conv2D)\n",
            "bn4d_branch2c          (BatchNorm)\n",
            "res4e_branch2a         (Conv2D)\n",
            "bn4e_branch2a          (BatchNorm)\n",
            "res4e_branch2b         (Conv2D)\n",
            "bn4e_branch2b          (BatchNorm)\n",
            "res4e_branch2c         (Conv2D)\n",
            "bn4e_branch2c          (BatchNorm)\n",
            "res4f_branch2a         (Conv2D)\n",
            "bn4f_branch2a          (BatchNorm)\n",
            "res4f_branch2b         (Conv2D)\n",
            "bn4f_branch2b          (BatchNorm)\n",
            "res4f_branch2c         (Conv2D)\n",
            "bn4f_branch2c          (BatchNorm)\n",
            "res5a_branch2a         (Conv2D)\n",
            "bn5a_branch2a          (BatchNorm)\n",
            "res5a_branch2b         (Conv2D)\n",
            "bn5a_branch2b          (BatchNorm)\n",
            "res5a_branch2c         (Conv2D)\n",
            "res5a_branch1          (Conv2D)\n",
            "bn5a_branch2c          (BatchNorm)\n",
            "bn5a_branch1           (BatchNorm)\n",
            "res5b_branch2a         (Conv2D)\n",
            "bn5b_branch2a          (BatchNorm)\n",
            "res5b_branch2b         (Conv2D)\n",
            "bn5b_branch2b          (BatchNorm)\n",
            "res5b_branch2c         (Conv2D)\n",
            "bn5b_branch2c          (BatchNorm)\n",
            "res5c_branch2a         (Conv2D)\n",
            "bn5c_branch2a          (BatchNorm)\n",
            "res5c_branch2b         (Conv2D)\n",
            "bn5c_branch2b          (BatchNorm)\n",
            "res5c_branch2c         (Conv2D)\n",
            "bn5c_branch2c          (BatchNorm)\n",
            "fpn_c5p5               (Conv2D)\n",
            "fpn_c4p4               (Conv2D)\n",
            "fpn_c3p3               (Conv2D)\n",
            "fpn_c2p2               (Conv2D)\n",
            "fpn_p5                 (Conv2D)\n",
            "fpn_p2                 (Conv2D)\n",
            "fpn_p3                 (Conv2D)\n",
            "fpn_p4                 (Conv2D)\n",
            "In model:  rpn_model\n",
            "    rpn_conv_shared        (Conv2D)\n",
            "    rpn_class_raw          (Conv2D)\n",
            "    rpn_bbox_pred          (Conv2D)\n",
            "mrcnn_mask_conv1       (TimeDistributed)\n",
            "mrcnn_mask_bn1         (TimeDistributed)\n",
            "mrcnn_mask_conv2       (TimeDistributed)\n",
            "mrcnn_mask_bn2         (TimeDistributed)\n",
            "mrcnn_class_conv1      (TimeDistributed)\n",
            "mrcnn_class_bn1        (TimeDistributed)\n",
            "mrcnn_mask_conv3       (TimeDistributed)\n",
            "mrcnn_mask_bn3         (TimeDistributed)\n",
            "mrcnn_class_conv2      (TimeDistributed)\n",
            "mrcnn_class_bn2        (TimeDistributed)\n",
            "mrcnn_mask_conv4       (TimeDistributed)\n",
            "mrcnn_mask_bn4         (TimeDistributed)\n",
            "mrcnn_bbox_fc          (TimeDistributed)\n",
            "mrcnn_mask_deconv      (TimeDistributed)\n",
            "mrcnn_class_logits     (TimeDistributed)\n",
            "mrcnn_mask             (TimeDistributed)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
            "  UserWarning('Using a generator with `use_multiprocessing=True`'\n",
            "Epoch 21/40\n",
            "2019-06-26 22:30:11.084653: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.30GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            "  1/105 [..............................] - ETA: 3:32:54 - loss: 0.7840 - rpn_class_loss: 0.0350 - rpn_bbox_loss: 0.2517 - mrcnn_class_loss: 0.1733 - mrcnn_bbox_loss: 0.1001 - mrcnn_mask_loss: 0.22402019-06-26 22:30:17.261572: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.71GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            "2019-06-26 22:30:17.329647: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.30GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            "  2/105 [..............................] - ETA: 1:48:09 - loss: 0.7400 - rpn_class_loss: 0.0354 - rpn_bbox_loss: 0.2029 - mrcnn_class_loss: 0.1894 - mrcnn_bbox_loss: 0.0953 - mrcnn_mask_loss: 0.21702019-06-26 22:30:20.329975: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.30GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            "  3/105 [..............................] - ETA: 1:13:09 - loss: 0.7437 - rpn_class_loss: 0.0305 - rpn_bbox_loss: 0.2179 - mrcnn_class_loss: 0.1859 - mrcnn_bbox_loss: 0.1015 - mrcnn_mask_loss: 0.20792019-06-26 22:30:23.489924: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.71GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            "  5/105 [>.............................] - ETA: 45:17 - loss: 0.7397 - rpn_class_loss: 0.0244 - rpn_bbox_loss: 0.2115 - mrcnn_class_loss: 0.1797 - mrcnn_bbox_loss: 0.1150 - mrcnn_mask_loss: 0.20912019-06-26 22:30:30.475678: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.30GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            "  6/105 [>.............................] - ETA: 38:17 - loss: 0.7416 - rpn_class_loss: 0.0266 - rpn_bbox_loss: 0.2072 - mrcnn_class_loss: 0.1763 - mrcnn_bbox_loss: 0.1184 - mrcnn_mask_loss: 0.21322019-06-26 22:30:39.667331: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.30GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            " 10/105 [=>............................] - ETA: 38:46 - loss: 0.7318 - rpn_class_loss: 0.0275 - rpn_bbox_loss: 0.2139 - mrcnn_class_loss: 0.1702 - mrcnn_bbox_loss: 0.1171 - mrcnn_mask_loss: 0.2032tcmalloc: large alloc 1150525440 bytes == 0xfd57c000 @  0x7f142d102001 0x7f142956fde5 0x7f14295d46f1 0x7f14295d67cf 0x7f142966f158 0x4f8925 0x4fa6c0 0x4f6128 0x4f7d60 0x4f876d 0x4fa6c0 0x4f6128 0x4f7d60 0x4f876d 0x4fa6c0 0x4f6128 0x4f7d60 0x4f876d 0x4f98c7 0x4f6128 0x4f7d60 0x4f876d 0x4fa6c0 0x59c333 0x5017cf 0x4f858d 0x4f98c7 0x4f6128 0x56fe24 0x57c2fe 0x4facb1\n",
            "tcmalloc: large alloc 1150525440 bytes == 0xfd57c000 @  0x7f142d102001 0x7f142956fde5 0x7f14295d46f1 0x7f14295d67cf 0x7f142966f158 0x4f8925 0x4fa6c0 0x4f6128 0x4f7d60 0x4f876d 0x4fa6c0 0x4f6128 0x4f7d60 0x4f876d 0x4fa6c0 0x4f6128 0x4f7d60 0x4f876d 0x4f98c7 0x4f6128 0x4f7d60 0x4f876d 0x4fa6c0 0x59c333 0x5017cf 0x4f858d 0x4f98c7 0x4f6128 0x56fe24 0x57c2fe 0x4facb1\n",
            "105/105 [==============================] - 2106s 20s/step - loss: 0.7494 - rpn_class_loss: 0.0237 - rpn_bbox_loss: 0.1999 - mrcnn_class_loss: 0.1849 - mrcnn_bbox_loss: 0.1253 - mrcnn_mask_loss: 0.2155 - val_loss: 0.7947 - val_rpn_class_loss: 0.0295 - val_rpn_bbox_loss: 0.2242 - val_mrcnn_class_loss: 0.1935 - val_mrcnn_bbox_loss: 0.1387 - val_mrcnn_mask_loss: 0.2088\n",
            "Epoch 22/40\n",
            "105/105 [==============================] - 1121s 11s/step - loss: 0.7170 - rpn_class_loss: 0.0260 - rpn_bbox_loss: 0.1952 - mrcnn_class_loss: 0.1740 - mrcnn_bbox_loss: 0.1183 - mrcnn_mask_loss: 0.2036 - val_loss: 0.9762 - val_rpn_class_loss: 0.0323 - val_rpn_bbox_loss: 0.2612 - val_mrcnn_class_loss: 0.2635 - val_mrcnn_bbox_loss: 0.1678 - val_mrcnn_mask_loss: 0.2514\n",
            "Epoch 23/40\n",
            "105/105 [==============================] - 755s 7s/step - loss: 0.7055 - rpn_class_loss: 0.0208 - rpn_bbox_loss: 0.1807 - mrcnn_class_loss: 0.1784 - mrcnn_bbox_loss: 0.1181 - mrcnn_mask_loss: 0.2074 - val_loss: 1.2110 - val_rpn_class_loss: 0.0586 - val_rpn_bbox_loss: 0.3828 - val_mrcnn_class_loss: 0.2994 - val_mrcnn_bbox_loss: 0.1932 - val_mrcnn_mask_loss: 0.2769\n",
            "Epoch 24/40\n",
            "105/105 [==============================] - 806s 8s/step - loss: 0.7337 - rpn_class_loss: 0.0263 - rpn_bbox_loss: 0.1967 - mrcnn_class_loss: 0.1825 - mrcnn_bbox_loss: 0.1208 - mrcnn_mask_loss: 0.2074 - val_loss: 0.8564 - val_rpn_class_loss: 0.0402 - val_rpn_bbox_loss: 0.2674 - val_mrcnn_class_loss: 0.1964 - val_mrcnn_bbox_loss: 0.1343 - val_mrcnn_mask_loss: 0.2180\n",
            "Epoch 25/40\n",
            "105/105 [==============================] - 803s 8s/step - loss: 0.7041 - rpn_class_loss: 0.0234 - rpn_bbox_loss: 0.1840 - mrcnn_class_loss: 0.1727 - mrcnn_bbox_loss: 0.1179 - mrcnn_mask_loss: 0.2060 - val_loss: 0.8893 - val_rpn_class_loss: 0.0463 - val_rpn_bbox_loss: 0.2461 - val_mrcnn_class_loss: 0.2214 - val_mrcnn_bbox_loss: 0.1433 - val_mrcnn_mask_loss: 0.2321\n",
            "Epoch 26/40\n",
            "105/105 [==============================] - 781s 7s/step - loss: 0.7055 - rpn_class_loss: 0.0196 - rpn_bbox_loss: 0.1878 - mrcnn_class_loss: 0.1763 - mrcnn_bbox_loss: 0.1162 - mrcnn_mask_loss: 0.2055 - val_loss: 0.8393 - val_rpn_class_loss: 0.0246 - val_rpn_bbox_loss: 0.2273 - val_mrcnn_class_loss: 0.2129 - val_mrcnn_bbox_loss: 0.1449 - val_mrcnn_mask_loss: 0.2296\n",
            "Epoch 27/40\n",
            "105/105 [==============================] - 823s 8s/step - loss: 0.7029 - rpn_class_loss: 0.0225 - rpn_bbox_loss: 0.1840 - mrcnn_class_loss: 0.1754 - mrcnn_bbox_loss: 0.1144 - mrcnn_mask_loss: 0.2066 - val_loss: 0.7789 - val_rpn_class_loss: 0.0273 - val_rpn_bbox_loss: 0.2028 - val_mrcnn_class_loss: 0.1974 - val_mrcnn_bbox_loss: 0.1278 - val_mrcnn_mask_loss: 0.2237\n",
            "Epoch 28/40\n",
            "105/105 [==============================] - 717s 7s/step - loss: 0.7077 - rpn_class_loss: 0.0212 - rpn_bbox_loss: 0.1949 - mrcnn_class_loss: 0.1756 - mrcnn_bbox_loss: 0.1156 - mrcnn_mask_loss: 0.2004 - val_loss: 0.7746 - val_rpn_class_loss: 0.0259 - val_rpn_bbox_loss: 0.1974 - val_mrcnn_class_loss: 0.2053 - val_mrcnn_bbox_loss: 0.1328 - val_mrcnn_mask_loss: 0.2133\n",
            "Epoch 29/40\n",
            "105/105 [==============================] - 796s 8s/step - loss: 0.7185 - rpn_class_loss: 0.0241 - rpn_bbox_loss: 0.1844 - mrcnn_class_loss: 0.1841 - mrcnn_bbox_loss: 0.1188 - mrcnn_mask_loss: 0.2071 - val_loss: 0.8701 - val_rpn_class_loss: 0.0241 - val_rpn_bbox_loss: 0.2439 - val_mrcnn_class_loss: 0.2297 - val_mrcnn_bbox_loss: 0.1467 - val_mrcnn_mask_loss: 0.2257\n",
            "Epoch 30/40\n",
            "105/105 [==============================] - 811s 8s/step - loss: 0.7085 - rpn_class_loss: 0.0249 - rpn_bbox_loss: 0.1867 - mrcnn_class_loss: 0.1724 - mrcnn_bbox_loss: 0.1173 - mrcnn_mask_loss: 0.2072 - val_loss: 0.8986 - val_rpn_class_loss: 0.0257 - val_rpn_bbox_loss: 0.2507 - val_mrcnn_class_loss: 0.2439 - val_mrcnn_bbox_loss: 0.1507 - val_mrcnn_mask_loss: 0.2276\n",
            "Epoch 31/40\n",
            "105/105 [==============================] - 712s 7s/step - loss: 0.6985 - rpn_class_loss: 0.0215 - rpn_bbox_loss: 0.1852 - mrcnn_class_loss: 0.1747 - mrcnn_bbox_loss: 0.1137 - mrcnn_mask_loss: 0.2033 - val_loss: 0.7688 - val_rpn_class_loss: 0.0282 - val_rpn_bbox_loss: 0.2078 - val_mrcnn_class_loss: 0.1936 - val_mrcnn_bbox_loss: 0.1256 - val_mrcnn_mask_loss: 0.2135\n",
            "Epoch 32/40\n",
            "105/105 [==============================] - 743s 7s/step - loss: 0.6972 - rpn_class_loss: 0.0219 - rpn_bbox_loss: 0.1792 - mrcnn_class_loss: 0.1755 - mrcnn_bbox_loss: 0.1147 - mrcnn_mask_loss: 0.2058 - val_loss: 0.9926 - val_rpn_class_loss: 0.0435 - val_rpn_bbox_loss: 0.2501 - val_mrcnn_class_loss: 0.2712 - val_mrcnn_bbox_loss: 0.1639 - val_mrcnn_mask_loss: 0.2639\n",
            "Epoch 33/40\n",
            "105/105 [==============================] - 833s 8s/step - loss: 0.7249 - rpn_class_loss: 0.0260 - rpn_bbox_loss: 0.2015 - mrcnn_class_loss: 0.1732 - mrcnn_bbox_loss: 0.1174 - mrcnn_mask_loss: 0.2069 - val_loss: 0.6543 - val_rpn_class_loss: 0.0237 - val_rpn_bbox_loss: 0.1601 - val_mrcnn_class_loss: 0.1667 - val_mrcnn_bbox_loss: 0.1180 - val_mrcnn_mask_loss: 0.1858\n",
            "Epoch 34/40\n",
            "105/105 [==============================] - 821s 8s/step - loss: 0.6986 - rpn_class_loss: 0.0200 - rpn_bbox_loss: 0.1777 - mrcnn_class_loss: 0.1772 - mrcnn_bbox_loss: 0.1168 - mrcnn_mask_loss: 0.2070 - val_loss: 0.9885 - val_rpn_class_loss: 0.0404 - val_rpn_bbox_loss: 0.2408 - val_mrcnn_class_loss: 0.2715 - val_mrcnn_bbox_loss: 0.1792 - val_mrcnn_mask_loss: 0.2566\n",
            "Epoch 35/40\n",
            "105/105 [==============================] - 847s 8s/step - loss: 0.6913 - rpn_class_loss: 0.0223 - rpn_bbox_loss: 0.1863 - mrcnn_class_loss: 0.1705 - mrcnn_bbox_loss: 0.1099 - mrcnn_mask_loss: 0.2023 - val_loss: 0.8890 - val_rpn_class_loss: 0.0399 - val_rpn_bbox_loss: 0.2059 - val_mrcnn_class_loss: 0.2400 - val_mrcnn_bbox_loss: 0.1571 - val_mrcnn_mask_loss: 0.2461\n",
            "Epoch 36/40\n",
            "105/105 [==============================] - 789s 8s/step - loss: 0.6985 - rpn_class_loss: 0.0209 - rpn_bbox_loss: 0.1761 - mrcnn_class_loss: 0.1773 - mrcnn_bbox_loss: 0.1177 - mrcnn_mask_loss: 0.2064 - val_loss: 0.8868 - val_rpn_class_loss: 0.0281 - val_rpn_bbox_loss: 0.2453 - val_mrcnn_class_loss: 0.2141 - val_mrcnn_bbox_loss: 0.1589 - val_mrcnn_mask_loss: 0.2404\n",
            "Epoch 37/40\n",
            "105/105 [==============================] - 737s 7s/step - loss: 0.6926 - rpn_class_loss: 0.0214 - rpn_bbox_loss: 0.1749 - mrcnn_class_loss: 0.1759 - mrcnn_bbox_loss: 0.1149 - mrcnn_mask_loss: 0.2054 - val_loss: 0.8740 - val_rpn_class_loss: 0.0395 - val_rpn_bbox_loss: 0.2010 - val_mrcnn_class_loss: 0.2231 - val_mrcnn_bbox_loss: 0.1589 - val_mrcnn_mask_loss: 0.2516\n",
            "Epoch 38/40\n",
            "105/105 [==============================] - 767s 7s/step - loss: 0.7005 - rpn_class_loss: 0.0196 - rpn_bbox_loss: 0.1835 - mrcnn_class_loss: 0.1739 - mrcnn_bbox_loss: 0.1171 - mrcnn_mask_loss: 0.2064 - val_loss: 0.8848 - val_rpn_class_loss: 0.0314 - val_rpn_bbox_loss: 0.2297 - val_mrcnn_class_loss: 0.2458 - val_mrcnn_bbox_loss: 0.1488 - val_mrcnn_mask_loss: 0.2290\n",
            "Epoch 39/40\n",
            "105/105 [==============================] - 735s 7s/step - loss: 0.7001 - rpn_class_loss: 0.0214 - rpn_bbox_loss: 0.1870 - mrcnn_class_loss: 0.1755 - mrcnn_bbox_loss: 0.1127 - mrcnn_mask_loss: 0.2035 - val_loss: 0.9185 - val_rpn_class_loss: 0.0260 - val_rpn_bbox_loss: 0.2634 - val_mrcnn_class_loss: 0.2258 - val_mrcnn_bbox_loss: 0.1491 - val_mrcnn_mask_loss: 0.2542\n",
            "Epoch 40/40\n",
            "105/105 [==============================] - 663s 6s/step - loss: 0.6943 - rpn_class_loss: 0.0197 - rpn_bbox_loss: 0.1801 - mrcnn_class_loss: 0.1759 - mrcnn_bbox_loss: 0.1145 - mrcnn_mask_loss: 0.2042 - val_loss: 0.7837 - val_rpn_class_loss: 0.0211 - val_rpn_bbox_loss: 0.2116 - val_mrcnn_class_loss: 0.1980 - val_mrcnn_bbox_loss: 0.1299 - val_mrcnn_mask_loss: 0.2231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3UcEbmf6HUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp Mask_RCNN/weights/kaggle_bowl.h5 drive/My\\ Drive/kaggle_bowl_trained.h5"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}